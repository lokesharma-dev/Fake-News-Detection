{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08.07.20_VAT",
      "provenance": [],
      "mount_file_id": "1_sJv6sgiLQ2YtWOfdHj8dFmQGj0ecjQW",
      "authorship_tag": "ABX9TyPrXb32G5ra1lqAKKuRS7Hr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokesharma-dev/Fake-News-Detection/blob/master/08_07_20_VAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PycGQ2yvsI0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9f396db2-bfa8-487b-8fdd-1e6e55c9328a"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Activation\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7va6rjscpT",
        "colab_type": "text"
      },
      "source": [
        "Pre-processing:  To be replaced; this is just a sample version from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN3JhKzPsVON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    \n",
        "    ## Remove puncuation\n",
        "    text = text.translate(string.punctuation)\n",
        "    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    ## Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    ## Stemming\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaNC5PJCs2M4",
        "colab_type": "text"
      },
      "source": [
        "If Input is original text file here; but has worked for all npy tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqC6UR6Bs17L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "760860cf-3a2d-42f9-d540-073f995ceb69"
      },
      "source": [
        "path = '/content/drive/My Drive/Colab Notebooks/Imdb/x_train.txt'\n",
        "with open(path, 'r') as file:\n",
        "  x_train = file.readlines()\n",
        "\n",
        "df = pd.DataFrame(x_train, columns=['Subject'])\n",
        "print('Before cleaning',df.head())\n",
        "\n",
        "df['Subject'] = df['Subject'].map(lambda x: clean_text(x))\n",
        "print('After cleaning', df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before cleaning                                              Subject\n",
            "0  Working with one of the best Shakespeare sourc...\n",
            "1  Well...tremors I, the original started off in ...\n",
            "2  Ouch! This one was a bit painful to sit throug...\n",
            "3  I've seen some crappy movies in my life, but t...\n",
            "4  \"Carriers\" follows the exploits of two guys an...\n",
            "After cleaning                                              Subject\n",
            "0  work one best shakespear sourc film manag cred...\n",
            "1  well tremor origin start 1990 found movi quit ...\n",
            "2  ouch ! one bit pain sit through cute amus prem...\n",
            "3  i have seen crappi movi life one must among wo...\n",
            "4  carrier follow exploit two guy two gal stolen ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3V6CvIwtFGQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d1e6944e-65ca-4563-818f-8b02a4590f9b"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Subject'])\n",
        "train_sequences = tokenizer.texts_to_sequences(df['Subject'])\n",
        "x_train = pad_sequences(train_sequences, maxlen=300, padding='post')\n",
        "type(x_train), len(x_train)\n",
        "vocab = tokenizer.word_index\n",
        "print('Length of Vocabulary: ',len(vocab))\n",
        "\n",
        "path = '/content/drive/My Drive/Colab Notebooks/Imdb/y_train.npy'\n",
        "y_train = np.load(path).astype('int32')\n",
        "print('Labels:', type(y_train), len(y_train))\n",
        "\n",
        "\n",
        "inds = np.arange(x_train.shape[0])\n",
        "random.Random(1).shuffle(inds)\n",
        "data = x_train[inds]\n",
        "labels = y_train[inds]\n",
        "\n",
        "num_test_samples = int(0.2 * data.shape[0])\n",
        "print('Split ratio {}/{}:'.format(num_test_samples, data.shape[0]))\n",
        "x_train = data[:-num_test_samples]\n",
        "y_train = labels[:-num_test_samples]\n",
        "x_test = data[-num_test_samples:]\n",
        "y_test = labels[-num_test_samples:]\n",
        "print(\"Training size:\", x_train.shape, y_train.shape)\n",
        "print(\"Testing size:\", x_test.shape, y_test.shape)\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(len(x_train), seed=1, reshuffle_each_iteration=True)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.shuffle(len(x_test), seed=1, reshuffle_each_iteration=True)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Vocabulary:  52207\n",
            "Labels: <class 'numpy.ndarray'> 24999\n",
            "Split ratio 4999/24999:\n",
            "Training size: (20000, 300) (20000,)\n",
            "Testing size: (4999, 300) (4999,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXLizz_WtwqH",
        "colab_type": "text"
      },
      "source": [
        "Create an embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ZbNdTqtnw_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d26f394-a6d1-41c4-d646-26c446b33e62"
      },
      "source": [
        "EMBEDDING_FILE = '/content/drive/My Drive/Colab Notebooks/Imdb/glove.6B.50d.txt'\n",
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "embedding_matrix = np.zeros((len(vocab) + 1, 50))\n",
        "invalid = 0\n",
        "for word, index in vocab.items():\n",
        "  if index > len(vocab) - 1:\n",
        "    break\n",
        "  else:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[index] = embedding_vector\n",
        "    else:\n",
        "      embedding_matrix[index] = np.random.uniform(low=0.01, high=0.05, size=50)\n",
        "      invalid += 1\n",
        "print('Words not found in glove: ', invalid)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words not found in glove:  20951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmEm0t1ut7Dx",
        "colab_type": "text"
      },
      "source": [
        "EagerTensor VAT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT-2Qncat1Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_kld(p_logit, q_logit):\n",
        "  p = tf.nn.sigmoid(p_logit)\n",
        "  q = tf.nn.sigmoid(q_logit)\n",
        "  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)\n",
        "  return kl_score\n",
        "\n",
        "def createEmbedding(features):\n",
        "  seq = Input(shape=(300,))\n",
        "  emb = Embedding(input_dim=len(vocab)+1,\n",
        "                  output_dim = 50,\n",
        "                  weights = [embedding_matrix],\n",
        "                  trainable=False)(seq)\n",
        "  emb_model = Model(seq, emb)\n",
        "  clean_emb = emb_model(features)\n",
        "  return clean_emb\n",
        "\n",
        "def createModel(embedding_features):\n",
        "  emb_tensor = Input(shape=(300,50,))\n",
        "  hidden = LSTM(units=128)(emb_tensor)\n",
        "  output = Dense(units=32, activation='relu')(hidden)\n",
        "  model = Model(inputs=emb_tensor, outputs=output)\n",
        "  logits = model(embedding_features)\n",
        "  return emb_tensor, output, logits\n",
        "\n",
        "def calculateGradient(clean_features, noised_features):\n",
        "  with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    tape.watch(noised_features)\n",
        "    _, _, p_logit = createModel(clean_features)\n",
        "    _, _, p_logit_r = createModel(noised_features)\n",
        "    kl_score = compute_kld(p_logit, p_logit_r)\n",
        "    # print('KL score:', kl_score)\n",
        "  grads = tape.gradient(kl_score, noised_features)\n",
        "  return grads\n",
        "\n",
        "def custom_loss(vat_loss):\n",
        "  def loss(true, pred):\n",
        "    b_loss = binary_crossentropy(true, pred)\n",
        "    # print('Vat_loss:', type(vat_loss), vat_loss)\n",
        "    # print('B_loss:', type(b_loss), b_loss)\n",
        "    net_loss = tf.math.add(vat_loss, b_loss)\n",
        "    # net_loss = vat_loss\n",
        "    return net_loss\n",
        "  return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ymnRrut_sf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6df3b1dc-b6d9-4fd2-ef94-94e889bb13a1"
      },
      "source": [
        "epsilon = 0.01\n",
        "features, labels = next(iter(train_dataset))\n",
        "\n",
        "clean_features = createEmbedding(features)\n",
        "noised_features = tf.add(clean_features, epsilon)\n",
        "# noised_features = tf.add(noised_features, clean_features)\n",
        "print('Clean Embedding: ', type(clean_features), clean_features.shape) \n",
        "print('Noised Embedding: ', type(noised_features), noised_features.shape)\n",
        "\n",
        "clean_ip_tensor, clean_op_tensor, p_logit = createModel(clean_features)\n",
        "noise_ip_tensor, noise_op_tensor, p_logit_r = createModel(noised_features)\n",
        "print('P_Logit: ',type(p_logit), p_logit.shape) \n",
        "print('P_Logit_R: ',type(p_logit_r), p_logit_r.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean Embedding:  <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 300, 50)\n",
            "Noised Embedding:  <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 300, 50)\n",
            "P_Logit:  <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 32)\n",
            "P_Logit_R:  <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS3wi_7jvXVe",
        "colab_type": "text"
      },
      "source": [
        "Calculate Adversary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSUU6lMFvaIf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e7f01b63-badb-4685-e42e-dbbf1936bf1c"
      },
      "source": [
        "grads = calculateGradient(clean_features, noised_features)\n",
        "norm_ball = tf.math.l2_normalize(grads, axis=None, epsilon=1e-12, name=None)\n",
        "rvadv = (grads/norm_ball) * -1 # As per the paper Miyato et al\n",
        "vadv_features = tf.add(clean_features, rvadv)\n",
        "vat_ip_tensor, vat_op_tensor, q_logit = createModel(vadv_features)\n",
        "\n",
        "print('Adversarial Embedding: ', type(vadv_features), vadv_features.shape) \n",
        "print('Q_logit: ',type(q_logit), q_logit.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adversarial Embedding:  <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 300, 50)\n",
            "Q_logit:  <class 'tensorflow.python.framework.ops.EagerTensor'> (1024, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTbQDidBuklX",
        "colab_type": "text"
      },
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yErH4ggQulU7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "9935b6b0-6786-4c56-d11b-01ab8d87706a"
      },
      "source": [
        "p = Dense(units=1, activation='softmax')(clean_op_tensor) # Tensor\n",
        "print('Prediction: ', type(p), p)\n",
        "model = Model(inputs=clean_ip_tensor, outputs=p)\n",
        "model.summary()\n",
        "\n",
        "vat_loss = compute_kld(p_logit, p_logit_r)\n",
        "model.compile(optimizer='Adam', loss= custom_loss(vat_loss), metrics=['accuracy'])\n",
        "\n",
        "print('VAT Loss : ', type(vat_loss), vat_loss)\n",
        "model.fit(clean_features, labels, batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction:  <class 'tensorflow.python.framework.ops.Tensor'> Tensor(\"dense_5/Identity:0\", shape=(None, 1), dtype=float32)\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 300, 50)]         0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               91648     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 95,809\n",
            "Trainable params: 95,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "VAT Loss :  <class 'tensorflow.python.framework.ops.EagerTensor'> tf.Tensor(\n",
            "[-0.02090424 -0.02090424 -0.02090424 ... -0.02090424 -0.02090424\n",
            " -0.0207103 ], shape=(1024,), dtype=float32)\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.4981 - accuracy: 0.5078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97d29e9908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzX5MIyBv-xB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "b85e7da3-302e-4480-c35d-c1fd0fd5d2da"
      },
      "source": [
        "N_Epochs = 1\n",
        "epsilon = 0.1\n",
        "\n",
        "for epoch in range(N_Epochs):\n",
        "  print('Epoch No {} ------'.format(epoch+1))\n",
        "  batch_no = 1\n",
        "  for features, labels in train_dataset:\n",
        "    print('Batch No: ', batch_no)\n",
        "    batch_no += 1\n",
        "    clean_emb = createEmbedding(features)\n",
        "    noised_emb = tf.add(clean_emb, epsilon)\n",
        "\n",
        "    clean_ip_tensor, clean_op_tensor, p_logit = createModel(clean_emb)\n",
        "    _, _, p_logit_r = createModel(noised_emb)\n",
        "\n",
        "    grads = calculateGradient(clean_emb, noised_emb)\n",
        "    norm_ball = tf.stop_gradient(grads)\n",
        "    norm_ball = tf.math.l2_normalize(norm_ball, axis=None, epsilon=1e-12, name=None)\n",
        "    rvadv = (grads/norm_ball) * -1 # As per the paper Miyato et al\n",
        "    vadv_features = tf.add(clean_emb, rvadv)\n",
        "    vat_ip_tensor, vat_op_tensor, q_logit = createModel(vadv_features)\n",
        "    p = Dense(units=1, activation='softmax')(clean_op_tensor) # Tensor\n",
        "    model = Model(inputs=clean_ip_tensor, outputs=p)\n",
        "    p_logit_no_gd = tf.stop_gradient(p_logit)\n",
        "    vat_loss = compute_kld(p_logit_no_gd, q_logit)\n",
        "    model.compile(optimizer='Adam', loss = custom_loss(vat_loss), metrics=['accuracy'])\n",
        "    model.fit(clean_emb, labels, batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch No 1 ------\n",
            "Batch No:  1\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.5044 - accuracy: 0.4971\n",
            "Batch No:  2\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.5310 - accuracy: 0.4951\n",
            "Batch No:  3\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.1811 - accuracy: 0.4922\n",
            "Batch No:  4\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9215 - accuracy: 0.5088\n",
            "Batch No:  5\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.9600 - accuracy: 0.4902\n",
            "Batch No:  6\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.0959 - accuracy: 0.4834\n",
            "Batch No:  7\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.4377 - accuracy: 0.5000\n",
            "Batch No:  8\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.2455 - accuracy: 0.4951\n",
            "Batch No:  9\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.6165 - accuracy: 0.5186\n",
            "Batch No:  10\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.5931 - accuracy: 0.4727\n",
            "Batch No:  11\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.8921 - accuracy: 0.4844\n",
            "Batch No:  12\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.4188 - accuracy: 0.5088\n",
            "Batch No:  13\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9984 - accuracy: 0.5146\n",
            "Batch No:  14\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.4090 - accuracy: 0.5078\n",
            "Batch No:  15\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.0717 - accuracy: 0.5098\n",
            "Batch No:  16\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.4455 - accuracy: 0.5078\n",
            "Batch No:  17\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.1883 - accuracy: 0.5156\n",
            "Batch No:  18\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.6427 - accuracy: 0.4961\n",
            "Batch No:  19\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.7278 - accuracy: 0.5107\n",
            "Batch No:  20\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.1145 - accuracy: 0.5055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUEiHELuUZyr",
        "colab_type": "text"
      },
      "source": [
        "Let's not consider the performance at this stage, as Model architecture is an abstract and this code-block only represents VAT. \n",
        "\n",
        "To Dos: \n",
        "\n",
        "Once Mean-Teacher's code is ready, and works for unlabelled examples. We move on to the final integration part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oRgckNmzwO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
