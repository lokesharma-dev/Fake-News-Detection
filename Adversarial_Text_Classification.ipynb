{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adversarial Text Classification",
      "provenance": [],
      "mount_file_id": "1sdv-sGE80HwdPKkl_p3gakhkxgKO-zRv",
      "authorship_tag": "ABX9TyNFovheUfGfvSrqHQsq744N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokesharma-dev/Fake-News-Detection/blob/master/Adversarial_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tj791IaWJNp",
        "colab_type": "text"
      },
      "source": [
        "# **Execute from here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnF-A8IMDH7y",
        "colab_type": "text"
      },
      "source": [
        "# Virtual Adversarial Training (Embedding Noise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAWyng7eC6ad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c31d0c3-285d-4299-e233-f41091e786e4"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "#------------------- Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\n",
        "\n",
        "word_index = 12186 # For above data, computed in some other codeblock\n",
        "MAX_VOCAB_SIZE = (word_index) + 1 # maximum no of unique words\n",
        "MAX_DOC_LENGTH = 500 # maximum no of words in each sentence\n",
        "EMBEDDING_DIM = 300 # Embeddings dimension from Glove directory\n",
        "print(tf.__version__)\n",
        "\n",
        "sequences = np.load('/content/drive/My Drive/Colab Notebooks/datasets/data.npy')\n",
        "y = np.load('/content/drive/My Drive/Colab Notebooks/datasets/label.npy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_XnfgRAzkox",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d9171911-ace5-49f6-a184-fb8d7c88b251"
      },
      "source": [
        "# Shuffle data random before splitting\n",
        "indices = np.arange(sequences.shape[0])\n",
        "random.Random(1).shuffle(indices)\n",
        "data = sequences[indices]\n",
        "labels = y[indices]\n",
        "\n",
        "num_test_samples = int(0.2 * data.shape[0])\n",
        "x_train = data[:-num_test_samples]\n",
        "y_train = labels[:-num_test_samples]\n",
        "x_test = data[-num_test_samples:]\n",
        "y_test = labels[-num_test_samples:]\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_train[0].shape, y_train[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400, 500) (400,)\n",
            "(500,) ()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJKsOZikDhq5",
        "colab_type": "text"
      },
      "source": [
        "Custom Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55vaPtzkiBeK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0795759-5110-4f76-ee02-994f8d0756e8"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((500,), ()), types: (tf.int32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU5H5DeGcGM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_kld(p_logit, q_logit):\n",
        "  p = tf.nn.softmax(p_logit)\n",
        "  q = tf.nn.softmax(q_logit)\n",
        "  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)\n",
        "  return kl_score # lower kl means closer the distributions are"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObTkEwMLikpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(40, seed=0, reshuffle_each_iteration=True)\n",
        "train_dataset = train_dataset.repeat(2)\n",
        "batched_dataset = train_dataset.batch(40)\n",
        "\n",
        "features, labels = next(iter(batched_dataset.take(4)))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pgMh9Wj_43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=MAX_VOCAB_SIZE,\n",
        "                           output_dim=EMBEDDING_DIM,\n",
        "                           input_length=MAX_DOC_LENGTH,\n",
        "                           name='Embedding_layer'))\n",
        "model.add(tf.keras.layers.LSTM(128))\n",
        "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(2))\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBLqT88BkQt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_layer_names=True, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk-wgwPCl-En",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "52690766-51ed-405a-dfb1-2766c944e547"
      },
      "source": [
        "pred = model(features[:10])\n",
        "pred_prob = tf.nn.softmax(pred)\n",
        "print(\"Prediction: {}\".format(tf.argmax(pred, axis=1)))\n",
        "print(\"    Labels: {}\".format(labels[:10]))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: [1 1 1 1 1 1 1 1 1 1]\n",
            "    Labels: [0 1 0 1 0 0 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vnrTeyVnH6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49bfb0f1-3736-45e6-979e-cda8fd97e8d9"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "def loss(model, x, y, training):\n",
        "  y_ = model(x, training=training)\n",
        "  return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "l = loss(model, features, labels, training=False)\n",
        "print(\"Loss test: {}\".format(l))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss test: 0.6925274729728699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9MRe3E6nUvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e1d01350-f3ad-4d25-e284-c0cff4be76d3"
      },
      "source": [
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets, training=True)\n",
        "  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss_value, grads = grad(model, features, labels)\n",
        "\n",
        "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "                                          loss_value.numpy()))\n",
        "\n",
        "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
        "                                          loss(model, features, labels, training=True).numpy()))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0, Initial Loss: 0.6925274729728699\n",
            "Step: 1,         Loss: 0.678128182888031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dSiDlYXn1F9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0e86def1-aa33-4db0-85b3-57ff1c535551"
      },
      "source": [
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "  for x, y in batched_dataset:\n",
        "    loss_value, grads = grad(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
        "    epoch_accuracy.update_state(y, model(x, training=True))\n",
        "\n",
        "  train_loss_results.append(epoch_loss_avg.result())\n",
        "  train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "  print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 000: Loss: 0.679, Accuracy: 52.125%\n",
            "Epoch 001: Loss: 0.665, Accuracy: 53.250%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10EW-FnJdv_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9d4afafc-0b00-4c17-d2af-41af2b95026f"
      },
      "source": [
        "noise = \n",
        "\n",
        "seq_input = tf.convert_to_tensor((x_train[0],))\n",
        "seq_input.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qv19SHHcKW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = keras.layers.Embedding(input_dim=MAX_VOCAB_SIZE,\n",
        "                           output_dim=EMBEDDING_DIM,\n",
        "                           name='Embedding_layer')(seq_input)\n",
        "                      \n",
        "h1 = layers.LSTM(units=128, name='LSTM_layer_v')(v)\n",
        "p_logit = layers.Dense(units=10, name='Dense_layer_v')(h1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMGMVr2CdBt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "a352567e-04c8-4fd3-d574-7d3a1755cd90"
      },
      "source": [
        "u"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[ 5.8567409e-40 -6.5531863e-40 -5.0462019e-40 ...  1.1564804e-39\n",
            "   -2.1842179e-40  2.3464322e-40]\n",
            "  [ 7.4412312e-40 -6.9397625e-40 -6.7859700e-40 ...  1.5409463e-39\n",
            "   -2.3563674e-40  4.0652789e-40]\n",
            "  [ 7.9690863e-40 -9.2688186e-40 -8.8635071e-40 ...  1.8805425e-39\n",
            "   -2.4893226e-40  4.1447886e-40]\n",
            "  ...\n",
            "  [ 7.3493793e-05  1.1440966e-04 -2.6020425e-04 ...  2.3355914e-04\n",
            "    8.3595383e-05  1.0019035e-04]\n",
            "  [ 9.6998221e-05  1.4272197e-04 -3.1448196e-04 ...  2.9717124e-04\n",
            "    1.0960357e-04  1.0925635e-04]\n",
            "  [ 1.3966946e-04  1.8355183e-04 -3.8214438e-04 ...  4.0356658e-04\n",
            "    1.3134073e-04  9.9243276e-05]]], shape=(1, 500, 300), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUp766Eqctzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC_LJGIkWDYP",
        "colab_type": "text"
      },
      "source": [
        "## VAT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxzOcXp7D2o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_emb = createEmbd(inputs)\n",
        "noise_emb = tf.random.uniform(shape=tf.shape(input_emb)) # Idea is to add noise to these embeddings\n",
        "#noise_emb = tf.math.add(input_emb, noise_emb)\n",
        "noise_emb = input_emb + noise_emb\n",
        "\n",
        "input_h1 = layers.LSTM(units=128,name=\"Input_h1\")(input_emb)\n",
        "noise_h1 = layers.LSTM(units=128,name=\"Noise_h1\")(noise_emb)\n",
        "\n",
        "p_logit = layers.Dense(units=16, activation='relu', name=\"p_logit\")(input_h1)\n",
        "p_logit_r = layers.Dense(units=16, activation='relu', name=\"p_logit_r\")(noise_h1)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "    tape.watch(noise_emb)\n",
        "    kl_score = compute_kld(p_logit, p_logit_r)\n",
        "    kl_score = tf.convert_to_tensor(kl_score, dtype=tf.float32)\n",
        "grads = tape.gradient(kl_score, noise_emb) # Differentiate kl_score with respect to noise_embd\n",
        "\n",
        "#p_logit = tf.stop_gradient(p_logit)\n",
        "#p_logit_r = tf.stop_gradient(p_logit_r)\n",
        "\n",
        "# Due to some reason the first execution returned \"None\" for gradients so manually added the shape to be able to build the model\n",
        "if grads is None:\n",
        "  grads = tf.random.uniform(shape=tf.shape(noise_emb)) \n",
        "\n",
        "vadv_emb = tf.math.add(input_emb, grads)\n",
        "vadv_h1 = layers.LSTM(units=128,name=\"vadv_h1\")(vadv_emb)\n",
        "q_logit = layers.Dense(units=16, activation='relu', name=\"q_logit\")(vadv_h1)\n",
        "\n",
        "vat_loss = compute_kld(p_logit, q_logit) # I need to add this vat loss(Scalar) to the final cost function\n",
        "\n",
        "# logits = layers.average([p_logit, p_logit_r, q_logit])\n",
        "outputs = layers.Dense(units=1, activation='softmax', name=\"output\")(q_logit)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.add_loss(vat_loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}