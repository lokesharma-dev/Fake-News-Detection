{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAT",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1swRgiYH83f5A9Y72DAt01TL1GhVO9gMh",
      "authorship_tag": "ABX9TyNCR8sSTobRJ9NbHUqGWgPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokesharma-dev/Fake-News-Detection/blob/master/VAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGLFHSPNlhdG",
        "colab_type": "text"
      },
      "source": [
        "## **Import Libararies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2ekX0CCY3BB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "#------------------- Text preprocessing\n",
        "import spacy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#------------------- Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vx4_rhAkTVa",
        "colab_type": "text"
      },
      "source": [
        "## **spaCy Preprocessing (Lokesh)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-nPsWGYj08k",
        "colab_type": "text"
      },
      "source": [
        "### Class Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgVs-56mj987",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Spacy(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # python -m spacy download en_core_web_lg\n",
        "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
        "        pass\n",
        "\n",
        "    def deNoise(self, text):\n",
        "        text = re.sub(r'[“”\"\"]', '', text) # removes quotes\n",
        "        text = text.replace(\"'s\", '')\n",
        "        text = re.sub(r'[-]', ' ', text) # helps in splitting doc into sentences\n",
        "        text = re.sub(r'http[\\w:/\\.]+', '', text) # removing urls\n",
        "        text = re.sub(r'[^\\.\\w\\s]', '', text) # removing everything but characters and punctuation\n",
        "        text = re.sub(r'\\.', '.', text) # replace periods with a single one\n",
        "        text = re.sub(r'\\n', ' ', text) # removing line break\n",
        "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "        text = re.sub(r'\\s\\s+', ' ', text)  # replace multiple whitespace with one\n",
        "        return text\n",
        "\n",
        "    def stopWords(self, text):\n",
        "        tokens = \"\"\n",
        "        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "        text = text.split(\" \")\n",
        "        for word in text:\n",
        "            if word not in spacy_stopwords:\n",
        "                tokens = tokens + \" \" + word\n",
        "        return tokens\n",
        "\n",
        "    def lemmatize(self, tokens):\n",
        "        lemma_token = \"\"\n",
        "        tokens_object = self.nlp(tokens)\n",
        "        # lemma_token = [token.lemma_ for token in tokens_object]\n",
        "        # lemma_token = ''.join(lemma_token) # converts list to string\n",
        "        for token in tokens_object:\n",
        "            lemma_token = lemma_token + \" \" + token.lemma_\n",
        "        lemma_token = re.sub(r'\\s\\s+', ' ', lemma_token)  # replace multiple whitespace with one\n",
        "        lemma_token = lemma_token.strip() # removes trailing whitespaces\n",
        "        return  lemma_token\n",
        "\n",
        "    def set_custom_boundaries(self, doc):\n",
        "        for token in doc[:-1]:\n",
        "            if token.text == '--':\n",
        "                doc[token.i+1].is_sent_start = True\n",
        "        return doc\n",
        "\n",
        "    def sentence_detect(self, text):\n",
        "        self.nlp.add_pipe(self.set_custom_boundaries, before='parser')\n",
        "        doc = self.nlp(text)\n",
        "        sentences = list(doc.sents)\n",
        "        for sentence in sentences:\n",
        "            print(sentence)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        print([token.text for token in doc])\n",
        "\n",
        "    def orchestrate(self, text):\n",
        "        return self.lemmatize(self.stopWords(self.deNoise(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEHw_qJ2j-nR",
        "colab_type": "text"
      },
      "source": [
        "### Class Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dzx6gvPj-6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedding(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # Parameters\n",
        "        self.MAX_VOCAB_SIZE = 1000000  # maximum no of unique words\n",
        "        self.MAX_DOC_LENGTH = 500  # maximum no of words in each sentence\n",
        "        self.EMBEDDING_DIM = 300  # Embeddings dimension from Glove directory\n",
        "        self.GLOVE_DIR = '/content/drive/My Drive/Colab Notebooks/glove/glove.6B/glove.6B.' + str(self.EMBEDDING_DIM) + 'd.txt'\n",
        "\n",
        "    def tokenize_padding(self, docs):\n",
        "        # Tokenize & pad sequences\n",
        "        tokenizer = Tokenizer(num_words=self.MAX_VOCAB_SIZE, oov_token='-EOS-')\n",
        "        tokenizer.fit_on_texts(docs)\n",
        "        encoded_docs = tokenizer.texts_to_sequences(docs)\n",
        "        word_index = tokenizer.word_index\n",
        "        print('Vocabulary size :', len(word_index))\n",
        "        sequences = pad_sequences(encoded_docs, padding='post', maxlen=self.MAX_DOC_LENGTH)\n",
        "        return [word_index, sequences]\n",
        "\n",
        "    def load_glove(self):\n",
        "        embeddings_index = {}\n",
        "        f = open(self.GLOVE_DIR, encoding='utf-8')\n",
        "        print('Loading Glove from: ', self.GLOVE_DIR, '...', end='')\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
        "        f.close()\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "        print('\\nDone.\\nProcedding with Embedded Matrix...', end='')\n",
        "        return embeddings_index\n",
        "\n",
        "    def embedding_matrix(self, word_index, embeddings_index):\n",
        "        # Create an embedding matrix\n",
        "        # first create a matrix of zeros, this is our embedding matrix\n",
        "        embeddings_matrix = np.zeros((len(word_index) + 1, self.EMBEDDING_DIM))\n",
        "        # embeddings_matrix = np.random.random(((20568),EMBEDDING_DIM))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                embeddings_matrix[i] = embedding_vector\n",
        "            else:\n",
        "                # doesn't exist, assign a random vector\n",
        "                embeddings_matrix[i] = np.random.random(self.EMBEDDING_DIM)\n",
        "        print('\\nCompleted')\n",
        "        return embeddings_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0D1xd90kpPj",
        "colab_type": "text"
      },
      "source": [
        "### __main__()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MukN5gyJktaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('GBVAT/data/processed_datasets/celebrityDataset.csv')\n",
        "\n",
        "# Feature Engineering\n",
        "df.nunique()\n",
        "df.isna().sum()\n",
        "df.Subject.fillna('', inplace=True)\n",
        "\n",
        "x = df.Subject + \" \" + df.Content\n",
        "y = pd.Series([0 if row == 'Fake' else 1 for row in df.Label])  # Series is 1D array but with same dtype\n",
        "\n",
        "S = Spacy()\n",
        "start = time.time()\n",
        "docs = [S.orchestrate(row) for row in x]\n",
        "end = time.time()\n",
        "print(\"Cleaning the document took {} seconds\".format(round(end - start)))\n",
        "\n",
        "E = Embedding()\n",
        "sequences = E.tokenize_padding(docs)\n",
        "word_index = sequences[0]\n",
        "sequences = sequences[1]\n",
        "print('Shape of data tensor:', sequences.shape)\n",
        "print('Shape of label tensor', y.shape)\n",
        "\n",
        "embeddings_index = E.load_glove()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ud9G4flTLg",
        "colab_type": "text"
      },
      "source": [
        "## **Load & Split Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87cESTWFZ3bT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data = np.load('/content/data.npy',allow_pickle=True)\n",
        "# label = np.load('/content/label.npy',allow_pickle=True)\n",
        "\n",
        "# Shuffle data random before splitting\n",
        "indices = np.arange(sequences.shape[0])\n",
        "random.Random(1).shuffle(indices)\n",
        "data = sequences[indices]\n",
        "labels = y[indices]\n",
        "\n",
        "num_test_samples = int(0.2 * data.shape[0])\n",
        "x_train = data[:-num_test_samples]\n",
        "y_train = labels[:-num_test_samples]\n",
        "x_test = data[-num_test_samples:]\n",
        "y_test = labels[-num_test_samples:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56xXksa3dlbo",
        "colab_type": "text"
      },
      "source": [
        "## **VAT Model (Lokesh)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVl9JU74jYw0",
        "colab_type": "text"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wtvzXobcC5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 1000000 # maximum no of unique words\n",
        "MAX_DOC_LENGTH = 500 # maximum no of words in each sentence\n",
        "EMBEDDING_DIM = 300 # Embeddings dimension from Glove directory\n",
        "\n",
        "inputs = Input(shape=(MAX_DOC_LENGTH,)) # TensorShape([None, 200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpTQJNvciFtq",
        "colab_type": "text"
      },
      "source": [
        "### Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7A4dDr8ay83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embeddingLayer():\n",
        "  network = Sequential()\n",
        "  network.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM))\n",
        "  p_logit = network(inputs) # TensorShape([None, 200, 128])\n",
        "  return [network, p_logit]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xul45ehCiUrz",
        "colab_type": "text"
      },
      "source": [
        "### User defined functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8K4Qvinb54t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_kld(p_logit, q_logit):\n",
        "  p = tf.nn.softmax(p_logit)\n",
        "  q = tf.nn.softmax(q_logit)\n",
        "  # kl_score = tf.reduce_sum(tf.where(condition=(p==0), x=tf.zeros(p.shape, tf.float64),y = p * tf.log(p/q)))\n",
        "  kl_score = tf.reduce_sum( p * (tf.math.log(p+1e-16) - tf.math.log(q+1e-16)), axis = 1)\n",
        "  return kl_score # lower kl means closer the distributions are\n",
        "# Plot p | q | kl\n",
        "# make sure p is not zero\n",
        "\n",
        "def make_unit_norm(x):\n",
        "  x_norm = x/(tf.reshape(tf.sqrt(tf.reduce_sum(tf.pow(x,2), axis=1)), [-1,1]) + 1e-16)\n",
        "  return x_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6C5JcE_iO0y",
        "colab_type": "text"
      },
      "source": [
        "### Vat Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsMDzyxxaYn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vatLoss():\n",
        "  returnList = embeddingLayer()\n",
        "  network = returnList[0]\n",
        "  p_logit = returnList[1]\n",
        "\n",
        "  # Random noise to be substituted in future with AM\n",
        "  r = tf.random.uniform(shape=tf.shape(inputs)) # TensorShape([None, 200])\n",
        "  r = make_unit_norm(r)\n",
        "  p_logit_r = network(inputs + 10*r) # Perturbations added just after Embedding layer \n",
        "  # TensorShape([None, 200, 128])\n",
        "\n",
        "  # with tf.GradientTape() as tape:\n",
        "  #   tape.watch(r)\n",
        "  #   r = tf.reduce_sum(r)\n",
        "  #   # p_logit_r = model_vat(inputs + 10*r)\n",
        "  #   kl_score = compute_kld(p_logit, p_logit_r)\n",
        "  # grads = tape.gradient(kl_score, r)\n",
        "  # grads.shape\n",
        "\n",
        "  kl_score = tf.reduce_mean(compute_kld(p_logit, p_logit_r)) # reduce_mean because kl scores have 128 dimensions from Embedding layer\n",
        "  tf.compat.v1.disable_eager_execution() # Fix this for future with tape\n",
        "  grads = tf.gradients(kl_score, r) # list\n",
        "  kl_grads = [grad if grad is not None else tf.zeros_like(r)for r, grad in zip([r], grads)][0] # TensorShape([None, 200])\n",
        "\n",
        "  # Adversarial perturbation\n",
        "  r_vadv = tf.stop_gradient(kl_grads)\n",
        "  r_vadv = make_unit_norm(r_vadv) # TensorShape([None, 200])\n",
        "  r_vadv = inputs  + r_vadv\n",
        "\n",
        "  # During GD don't train your logits\n",
        "  p_logit_no_gradient = tf.stop_gradient(p_logit) # same dimention as p_logit # TensorShape([None, 200, 128])\n",
        "\n",
        "  p_logit_r_adv = network(r_vadv) \n",
        "  \n",
        "  # KLD(p_logit_no_gradient|p_logit_r_adv)\n",
        "  vat_loss = tf.reduce_mean(compute_kld(p_logit_no_gradient, p_logit_r_adv)) # Scalar\n",
        "  return vat_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v2OklSK43nW",
        "colab_type": "text"
      },
      "source": [
        "### Variant 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3pSqRY1SWWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "returnList = embeddingLayer()\n",
        "p_logit = returnList[1]\n",
        "layer1 = Bidirectional(LSTM(units=128))(p_logit)\n",
        "layer2 = Dense(units=2, activation='sigmoid')(layer1)\n",
        "output_layer = Dense(units=1)(layer2)\n",
        "model = Model(inputs, output_layer)\n",
        "vat_loss = vatLoss()\n",
        "model.add_loss(vat_loss)\n",
        "# model.summary()\n",
        "# tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35zE8AEfjJQM",
        "colab_type": "text"
      },
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cimadt51eh17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),loss= 'binary_crossentropy',metrics=['accuracy'])\n",
        "model.metrics_names.append('vat_loss')\n",
        "model.metrics.append(vat_loss)\n",
        "model.fit(x_train, y_train, epochs= 3, validation_split=0.2, shuffle= True, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byuKnBkgeh6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "31585461-1fa9-42f5-a794-77072bac6dd9"
      },
      "source": [
        "score, acc = model.evaluate(x_test, y_test)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 7.777111825942993\n",
            "Test accuracy: 0.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX3yTfacrbT0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}